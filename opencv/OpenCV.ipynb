{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#img = cv2.imread('watch.jpg', 0)\n",
    "# 1 corresponds to IMREAD_COLOR -1 to IMREAD_UNCHANGED \n",
    "#cv2.IMREAD_GRAYSCALE removes the alpha channel from the image which is the degree of opaqueness\n",
    "# We convert to grayscale because it is much more simplified\n",
    "#In case of OpenCV image is read as BGR and not as RGB\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0) #waits for any key to be pressed to exit, necessary otherwise image will not wait on screen\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('watchgray.png',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0) #if you have multiple webcams in the system then the 0,1,2 specifies the webcam to use \n",
    " \n",
    "while(True):\n",
    "    ret, frame = cap.read() #ret returns true or false\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #cvtColor is used to convert the color \n",
    " \n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cap.release() #release the camera to be able to use it again\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "waitKey(0) will pause your screen because it will wait infinitely for keyPress on your keyboard and will not refresh the frame(cap.read()) using your WebCam. waitKey(1) will wait for keyPress for just 1 millisecond and it will continue to refresh and read frame from your webcam using cap.read(). waitKey() returns the value of the key pressed. ord() sends a unicode value like the ascii value for that character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  #The codec\n",
    "out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    out.write(frame)\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_COLOR)\n",
    "cv2.line(img,(0,0),(150,150),(255,255,255),15)\n",
    "#cv2.line(source image,start point,end point,color in bgr,thickness) 255 means maximum brightness of color\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_COLOR)\n",
    "cv2.rectangle(img,(15,25),(200,150),(0,0,255),15) #if you do -1 instead of 15 then it means fill \n",
    "#cv2.rectangle(source image,left bottom corner of rectangle,right top corner,color in bgr,thickness) 255 means maximum brightness of color\n",
    "cv2.circle(img,(100,63), 55, (0,255,0), -1)\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_COLOR)\n",
    "pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)\n",
    "cv2.polylines(img, [pts], True, (0,255,255), 3) #True means if you want to connect the final and initial points or not\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. A list is the Python equivalent of an array, but is resizeable and can contain elements of different types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_COLOR)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "cv2.putText(img,'OpenCV',(0,130), font, 1, (200,255,155), 2, cv2.LINE_AA)\n",
    "#cv2.putText(img,text,Starting point, font, size, color, spacing between digits, line type) Line_AA is for antialiasing i.e remove the stair like end pixels and make it smooth end pixels are made successively smooth and not abruptly\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_COLOR)\n",
    "px = img[55,55]\n",
    "print(px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[55,55] = [255,255,255]\n",
    "px = img[55,55]\n",
    "print(px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = img[100:150,100:150]\n",
    "print(roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img[100:150,100:150] = [255,255,255]\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_COLOR)\n",
    "watch_face = img[37:111,107:194]\n",
    "img[0:74,0:87] = watch_face\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 500 x 250\n",
    "img1 = cv2.imread('3D-Matplotlib.png')\n",
    "img2 = cv2.imread('mainsvmimage.png')\n",
    "\n",
    "add = img1+img2\n",
    "#This add adds the pixel value and if a pixel value is greater than 255 then 266 is subtracted from the sum\n",
    "#[55,166,177] + [56,66,197] = [111 232 118] and [240,200,165] + [40,125,200] = [255,255,255]\n",
    "cv2.imshow('img1',img2)\n",
    "cv2.imshow('img2',img1)\n",
    "cv2.imshow('add',add)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = cv2.add(img1,img2) # adds the pixel values and if greaeter than 255 then changed to 255\n",
    "#E.g [125,125,125] + [12,10,100] = [137,135,225] and [240,200,165] + [40,125,200] = [24,69,109]\n",
    "cv2.imshow('add',add)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = cv2.addWeighted(img1, 0.6, img2, 0.4, 0)\n",
    "cv2.imshow('weighted',weighted)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two images\n",
    "img1 = cv2.imread('3D-Matplotlib.png')\n",
    "img2 = cv2.imread('mainlogo.png')\n",
    "\n",
    "# I want to put logo on top-left corner, So I create a ROI\n",
    "rows,cols,channels = img2.shape\n",
    "roi = img1[0:rows, 0:cols]\n",
    "\n",
    "# Now create a mask of logo and create its inverse mask\n",
    "img2gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# add a threshold\n",
    "ret, mask = cv2.threshold(img2gray, 220, 255, cv2.THRESH_BINARY_INV) #the style depends on your usage\n",
    "\n",
    "#cv2.threshold(img, lower limit, max value, thresholding style) any value above lower limit is changed to the max value \n",
    "#and below that to 0. There are many types but we mostly use the binary threshold, others are truncation and TOZERO which \n",
    "#are not useful. INVERSE LITERALLY INVERTS THE COLORS\n",
    "# OTSZU THRESHOLDING STYLE IS QUITE INTERESTING, RARELY USED BUT SOLELY HAS THE POWER TO SOLVE CERTAIN TYPES OF PROBLEMS WHEN\n",
    "#THE IMAGE IS NOISY. YOU CAN READ IF YOU WANT.\n",
    "cv2.imshow('mask', mask)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two images\n",
    "img1 = cv2.imread('3D-Matplotlib.png')\n",
    "img2 = cv2.imread('mainlogo.png')\n",
    "\n",
    "# I want to put logo on top-left corner, So I create a ROI\n",
    "rows,cols,channels = img2.shape\n",
    "roi = img1[0:rows, 0:cols ]\n",
    "\n",
    "# Now create a mask of logo and create its inverse mask\n",
    "img2gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# add a threshold\n",
    "ret, mask = cv2.threshold(img2gray, 220, 255, cv2.THRESH_BINARY_INV) #As the colors in the image are not very bright therefore \n",
    "# a threshold of 220 will capture all of them\n",
    "\n",
    "mask_inv = cv2.bitwise_not(mask)\n",
    "cv2.imshow(\"mask\", mask)\n",
    "cv2.imshow(\"mask_inv\", mask_inv)\n",
    "# Now black-out the area of logo in ROI\n",
    "img1_bg = cv2.bitwise_and(roi,roi,mask = mask_inv)\n",
    "#Mask is binary, the binary operation is operated only when mask is 1 i.e white\n",
    "cv2.imshow(\"img1_bg\", img1_bg)\n",
    "# Take only region of logo from logo image.\n",
    "img2_fg = cv2.bitwise_and(img2,img2,mask = mask)\n",
    "cv2.imshow(\"img2_fg\", img2_fg)\n",
    "dst = cv2.add(img1_bg,img2_fg)\n",
    "img1[0:rows, 0:cols ] = dst\n",
    "\n",
    "cv2.imshow('res',img1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread('bookpage.jpg')\n",
    "retval, threshold = cv2.threshold(img, 12, 255, cv2.THRESH_BINARY) \n",
    "#as we have applied threshold on colored image therefore each color channel ie BGR are converted to 255 or 0\n",
    "cv2.imshow('original',img)\n",
    "cv2.imshow('threshold',threshold)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grayscaled = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "retval, threshold = cv2.threshold(grayscaled, 10, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow('threshold',threshold)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = cv2.adaptiveThreshold(grayscaled, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 2)\n",
    "#cv2.adaptiveThreshold(src, maxValue, adaptiveMethod, thresholdType, blockSize, C) c is the const which is subtracted\n",
    "cv2.imshow('Adaptive threshold',th)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't specified threshold beacuse the method specifies a local threshold on its own.\n",
    "For the method ADAPTIVE_THRESH_MEAN_C , the threshold value T(x,y) is a mean of the blocksize X blocksize neighborhood of (x, y) minus C .\n",
    "For the method ADAPTIVE_THRESH_GAUSSIAN_C , the threshold value T(x, y) is a weighted sum (cross-correlation with a Gaussian window) of the blocksize X blocksize neighborhood of (x, y) minus C . The default sigma (standard deviation) is used for the specified blockSize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([150,150,50])\n",
    "    upper_red = np.array([200,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    cv2.imshow('mask',mask)\n",
    "    cv2.imshow('res',res)\n",
    "    \n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change to hsv so as to specify a range\n",
    "![alt text](123.png \"Title\")\n",
    "Hue -> Color \n",
    "\n",
    "Saturation -> Intensity\n",
    "\n",
    "Value -> Darkness\n",
    "![alt text](145.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([150,150,50])\n",
    "    upper_red = np.array([200,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "    kernel = np.ones((15,15),np.float32)/225\n",
    "    smoothed = cv2.filter2D(res,-1,kernel)\n",
    "    #cv2.filter2D(src, ddepth, kernel) ddepth is the depth of the output image, -1 means to keep the depth of output same as the src image\n",
    "    #The operation works like this: keep this kernel above a pixel, add all the 25 pixels below this kernel, take the \n",
    "    #average, and replace the central pixel with the new average value. This is essentially a low pass filter for an image\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('res',res)\n",
    "    cv2.imshow('Averaging',smoothed)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([150,150,50])\n",
    "    upper_red = np.array([200,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "    smoothed= cv2.blur(res,(3,3))\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('res',res)\n",
    "    cv2.imshow('Averaging',smoothed)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([150,150,50])\n",
    "    upper_red = np.array([200,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "    blur = cv2.GaussianBlur(res,(15,15),0) #here (15,15) is the kernel size and 0 is the sigma/std deviation\n",
    "    cv2.imshow('Gaussian Blurring',blur)\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('res',res)\n",
    "    \n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([150,150,50])\n",
    "    upper_red = np.array([200,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "    median = cv2.medianBlur(res,15) #15 is the kernel size\n",
    "    cv2.imshow('Median Blur',median)\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('res',res)\n",
    "    \n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([150,150,50])\n",
    "    upper_red = np.array([200,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "    bilateral = cv2.bilateralFilter(res,15,75,75)\n",
    "    cv2.imshow('bilateral Blur',bilateral)\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('res',res)\n",
    "    \n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.bilateralFilter(src, d, sigmaColor, sigmaSpace)\n",
    "\n",
    "d is the diameter of each pixel neighbourhood which is used in filtering\n",
    "\n",
    "sigmaColor – Filter sigma in the color space. A larger value of the parameter means that farther colors within \n",
    "the pixel neighborhood (see sigmaSpace ) will be mixed together, resulting in larger areas of semi-equal color.\n",
    "\n",
    "sigmaSpace – Filter sigma in the coordinate space. A larger value of the parameter means that farther pixels will \n",
    "influence each other as long as their colors are close enough (see sigmaColor ). When d>0 , it specifies the \n",
    "neighborhood size regardless of sigmaSpace . Otherwise, d is proportional to sigmaSpace ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([150,150,50])\n",
    "    upper_red = np.array([200,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    erosion = cv2.erode(mask,kernel,iterations = 1)\n",
    "    dilation = cv2.dilate(mask,kernel,iterations = 1)\n",
    "\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('Mask',mask)\n",
    "    cv2.imshow('Erosion',erosion)\n",
    "    cv2.imshow('Dilation',dilation)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erosion is where we will \"erode\" the edges. The way these work is we work with a slider (kernel). We give the slider a size, let's say 5 x 5 pixels. What happens is we slide this slider around, and if all of the pixels are white, then we get white, otherwise black. This may help eliminate some white noise. The other version of this is Dilation, which basically does the opposite: Slides around, if the entire area isn't black, then it is converted to white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([150,150,50])\n",
    "    upper_red = np.array([200,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    \n",
    "    opening = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('Mask',mask)\n",
    "    cv2.imshow('Opening',opening)\n",
    "    cv2.imshow('Closing',closing)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal with opening is to remove \"false positives\" so to speak. Sometimes, in the background, you get some pixels here and there of \"noise.\" The idea of \"closing\" is to remove false negatives. Basically this is where you have your detected shape, like our object, and yet you still have some black pixels within the object. Closing will attempt to clear that up.\n",
    "\n",
    "MORPH_OPEN -> dilate(erode(src frame))\n",
    "\n",
    "MORPH_CLOSE -> erode(dilate(src frame))\n",
    "\n",
    "MORPH_GRAD -> dilate(src frame) - erode(src frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    cv2.imshow('Original',frame)\n",
    "    edges = cv2.Canny(frame,100,200)\n",
    "    cv2.imshow('Edges',edges)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.Canny(src frame, threshold 1, threshold 2)\n",
    "\n",
    "The smallest value between threshold1 and threshold2 is used for edge linking. The largest value is used to find initial \n",
    "segments of strong edges. This function is based on The canny edge detection algortihm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('watch.jpg')\n",
    "\n",
    "res = cv2.resize(img,None,fx=2, fy=2, interpolation = cv2.INTER_CUBIC) #bicubic interpolation over 4x4 pixel neighborhood\n",
    "#fx and fy are scale factors along x and y axis you can also do res = cv2.resize(img,(2*width, 2*height), interpolation = cv2.INTER_CUBIC)\n",
    "cv2.imshow('asd', img)\n",
    "cv2.imshow('img',res)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation\n",
    "Translation is the shifting of object’s location. If you know the shift in (x,y) direction, let it be (t_x,t_y), you can create the transformation matrix M as follows:\n",
    "\n",
    "M = \\begin{bmatrix} 1 & 0 & t_x \\\\ 0 & 1 & t_y  \\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('watch.jpg',0)\n",
    "rows,cols = img.shape\n",
    "\n",
    "M = np.float32([[1,0,100],[0,1,50]])\n",
    "dst = cv2.warpAffine(img,M,(cols,rows)) #cols, rows is the size of output image\n",
    "#warp affine basically applies the matrix M onto the image\n",
    "cv2.imshow('asd', img)\n",
    "cv2.imshow('img',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('watch.jpg',0)\n",
    "rows,cols = img.shape\n",
    "\n",
    "M = cv2.getRotationMatrix2D((cols/2,rows/2),90,1)\n",
    "#cv2.getRotationMatrix2D(center,angle,scale)\n",
    "dst = cv2.warpAffine(img,M,(cols, rows))\n",
    "cv2.imshow('asd', img)\n",
    "cv2.imshow('img',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('watch.jpg')\n",
    "rows,cols,ch = img.shape\n",
    "\n",
    "pts1 = np.float32([[50,50],[200,50],[50,200]])\n",
    "pts2 = np.float32([[10,100],[200,50],[100,250]])\n",
    "\n",
    "M = cv2.getAffineTransform(pts1,pts2)\n",
    "\n",
    "dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "cv2.imshow('asd', img)\n",
    "\n",
    "cv2.imshow('img',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('sudoku.png')\n",
    "rows,cols,ch = img.shape\n",
    "\n",
    "pts1 = np.float32([[72,85],[480,72],[40,520],[510,520]])\n",
    "pts2 = np.float32([[0,0],[500,0],[0,500],[500,500]])\n",
    "\n",
    "M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "\n",
    "dst = cv2.warpPerspective(img,M,(500,500))\n",
    "cv2.imshow('asd', img)\n",
    "\n",
    "cv2.imshow('img',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](perspective.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Contours is: 1\n",
      "[[[-1 -1 -1 -1]]]\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"contour.jpg\")\n",
    "# Find edges\n",
    "edges = cv2.Canny(image, 10, 100)\n",
    "# Find Contours\n",
    "contours, hierarchy = cv2.findContours(edges,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "#Many diff types of modes and methods which you can explore later, retr_external outputs only the external contours \n",
    "#with a single level of hierarchy whereas there's retr_ccomp which outputs all contours with a two level hierarchy\n",
    "print(\"Number of Contours is: \" + str(len(contours)))\n",
    "print(hierarchy)\n",
    "cv2.drawContours(image, contours, -1, (0, 230, 255), 3)\n",
    "#cv2.drawContours(image, contours, index of contour -1 means all, color, thickness)\n",
    "cv2.imshow('Contours', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.findContours(image, mode, method) Each contour is stored as a vector of points.\n",
    "The input image can only be an 8 bit single channel image therefore you can use compare() , inRange() , threshold() , adaptiveThreshold() , Canny() etc for conversion \n",
    "\n",
    "##### hierarchy – \n",
    "Optional output vector, containing information about the image topology. It has as many elements as the number of contours. For each i-th contour contours[i] , the elements hierarchy[i][0] , hiearchy[i][1] , hiearchy[i][2] , and hiearchy[i][3] are set to 0-based indices in contours of the next and previous contours at the same hierarchical level, the first child contour and the parent contour, respectively. If for the contour i there are no next, previous, parent, or nested contours, the corresponding elements of hierarchy[i] will be negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"fitline.jpg\")\n",
    "# Find edges\n",
    "edges = cv2.Canny(image, 10, 100)\n",
    "x,y,w,h = cv2.boundingRect(edges)\n",
    "#similar to contours this also requires a single channel image (essentialy a set of pts) therefore we apply canny. It gives\n",
    "#us one corner x,y and width & height of rectangle which is of min size and encompass all the points.\n",
    "img = cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "#cv2.rectangle(image, (x,y), (x+w,y+h), color, thickness)\n",
    "cv2.imshow('Contours', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another feature called convexHull, it will give out a minimum bounding arbitary shape like if you want a min bound of your hand.\n",
    "![alt text](conv.jpg \"Title\") \n",
    "The arrows in this picture depict the convexity defects\n",
    "\n",
    "cv2.isContourConvex is used to find if the contour is convex or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = cv2.imread(\"fitline.jpg\")\n",
    "# Find edges\n",
    "edges = cv2.Canny(image, 150, 200)\n",
    "contours, hierarchy = cv2.findContours(edges,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "(x,y),radius = cv2.minEnclosingCircle(contours[1])\n",
    "#similarly a min area rectangle, ellipse functions are also there. \n",
    "#for minimum bounding rect or min bounding circle a matrix/numpy array is required to you have to input a contour, edges etc\n",
    "#can't be used\n",
    "center = (int(x),int(y))\n",
    "radius = int(radius)\n",
    "img = cv2.circle(image,center,radius,(0,255,0),2)\n",
    "cv2.imshow('Contours', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread(\"fitline.jpg\")\n",
    "# Find edges\n",
    "edges = cv2.Canny(image, 150, 200)\n",
    "contours, hierarchy = cv2.findContours(edges,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "rect = cv2.minAreaRect(contours[1])\n",
    "\n",
    "box = cv2.boxPoints(rect)\n",
    "box = np.int0(box)\n",
    "im = cv2.drawContours(image,[box],0,(0,0,255),2)\n",
    "cv2.imshow('Contours', im)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of minarearect is in the form (x-coordinate, y-coordinate),(width, height), rotation\n",
    "boxPoints converts the output into vertices of rectangle and int0 converts them to integers and therefore we get a contour type input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "planets= cv2.imread('planet.jpg')\n",
    "gray_img=cv2.cvtColor(planets,cv2.COLOR_BGR2GRAY)\n",
    "img= cv2.medianBlur(gray_img,5)\n",
    "\n",
    "circles= cv2.HoughCircles(img,cv2.HOUGH_GRADIENT,1,120,param1=100,param2=30,minRadius=10,maxRadius=0)\n",
    "circles= np.uint16(np.around(circles))\n",
    "for i in circles[0,:]:\n",
    "\n",
    "    cv2.circle(planets,(i[0],i[1]),i[2],(0,255,0),6)\n",
    "\n",
    "    cv2.circle(planets,(i[0],i[1]),2,(0,0,255),3)\n",
    "\n",
    "cv2.imshow(\"HoughCirlces\",planets)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.HoughCircles(image, method, dp, minDist, circles, param1, param2, minRadius, maxRadius) <br />\n",
    "<b>dp</b> = accumulator_resolution/image_resolution <br />\n",
    "<b>minDist</b>= min distance between two detected circles. If the parameter is too small, multiple neighbor circles may be falsely detected in addition to a true one. If it is too large, some circles may be missed. <br />\n",
    "<b>Circles</b>- Output vector<br />\n",
    "<b>param1</b> – First method-specific parameter. In case of CV_HOUGH_GRADIENT , it is the higher threshold of the two passed to the Canny() edge detector (the lower one is twice smaller). <br />\n",
    "<b>param2</b> – Second method-specific parameter. In case of CV_HOUGH_GRADIENT , it is the accumulator threshold for the circle centers at the detection stage. The smaller it is, the more false circles may be detected. Circles, corresponding to the larger accumulator values, will be returned first. <br />\n",
    "<b>minRadius </b>– Minimum circle radius. <br />\n",
    "<b>maxRadius</b> – Maximum circle radius.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('sudoku.png')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "edges = cv2.Canny(gray,100,200,apertureSize = 3)\n",
    "cv2.imshow('edges',edges)\n",
    "\n",
    "\n",
    "minLineLength = 1\n",
    "maxLineGap = 1000\n",
    "lines = cv2.HoughLinesP(edges,1,np.pi/180,150,minLineLength=minLineLength,maxLineGap=maxLineGap)\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        cv2.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "\n",
    "cv2.imshow('hough',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.HoughLinesP(image, rho, theta, threshold, lines, minLineLength, maxLineGap)\n",
    "P stands for probablistic and this is the optimised version of HoughLines and is faster,easier to use and more effecient\n",
    "input should be an 8 bit single channel image<br />\n",
    "<b>rho</b> – Distance resolution of the accumulator in pixels.Low values of rho give precision <br />\n",
    "<b>theta</b> – Angle resolution of the accumulator in radians. better to keep pi/180 i.e 1 degree.<br />\n",
    "<b>threshold</b> – Accumulator threshold parameter. Only those lines are returned that get enough votes .threshold. <br />\n",
    "<b>minLineLength</b> – Minimum line length. Line segments shorter than that are rejected. <br />\n",
    "<b>maxLineGap</b> – Maximum allowed gap between points on the same line to link them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('openbg1.mp4')\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    fgmask = fgbg.apply(frame)\n",
    " \n",
    "    cv2.imshow('fgmask',frame)\n",
    "    cv2.imshow('frame',fgmask)\n",
    "\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cascade Classifier\n",
    "\n",
    "Haar Cascade is a machine learning object detection algorithm used to identify objects in an image or video and based on the concept of ​​ features proposed by Paul Viola and Michael Jones\n",
    "https://github.com/opencv/opencv/tree/master/data/haarcascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while 1:\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 1)\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, 1.3)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "    cv2.imshow('img',img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.CascadeClassifier.detectMultiScale(image, scaleFactor, minNeighbors)\n",
    "scale means that the image is scaled by this amount for optimised detection but if it is very high then there can be issues like if you bring your face close to the camera then it will not be able to detect your face because it is seeing the frame as 3X. Also these are optional parameters only image is necessary as you can see in the eye detection\n",
    "\n",
    "minNeighbors – Parameter specifying how many neighbors each candidate rectangle should have to retain it.\n",
    "Higher value means less number of detections but with higher quality i.e less number of false positives.\n",
    "\n",
    "Very useful and can be used to create multiple projects like blink rate detection, face mask detection etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are many other features like template matching, grabcut, corner detection etc which I didn't include due to time constraint and their level of complexity. If you want to explore more about them then you can refer to the extra links we will share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                           <center><----- THANK YOU-----> </center>                      \n",
    "    \n",
    "Credits- Bhuvan Aggarwal (190040026)<br />\n",
    "For any doubts or future resources feel free to contact me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
